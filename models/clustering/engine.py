import logging
from typing import Any, Dict, List, Optional
from collections.abc import Mapping, Sequence

import hdbscan
import numpy as np
from sklearn.preprocessing import StandardScaler
from umap import UMAP

from core.data_models import ClusteringResult
from core.exceptions import ModelTrainingError, PredictionError, ValidationError
from .grid_search import GridSearchResult, HDBSCANGridSearch
from .knn_predictor import KNNPredictor

logger = logging.getLogger(__name__)


class AdaptiveClusteringEngine:
    """
    Adaptive clustering engine using HDBSCAN with UMAP preprocessing and kNN prediction.

    The workflow:
        1. Standardize input features.
        2. Reduce dimensionality with UMAP (1536 â†’ 128 by default).
        3. Run an HDBSCAN grid search, logging each attempt to MLflow.
        4. Train a kNN prediction model on the reduced embedding for future assignments.
    """

    def __init__(
        self,
        hdbscan_params: Dict[str, Any],
        knn_params: Dict[str, Any],
        umap_params: Dict[str, Any],
        clustering_param_grid: Optional[Mapping[str, Sequence[Any]] | Sequence[Mapping[str, Any]]] = None,
    ) -> None:
        # HDBSCAN clustering method
        self.hdbscan_params = hdbscan_params.copy()
        self.hdbscan_params['prediction_data'] = False
        self.hdbscan_model: Optional[hdbscan.HDBSCAN] = None

        # kNN supervised method
        self.knn_params = knn_params.copy()
        self.knn_predictor = KNNPredictor(self.knn_params)

        # UMAP dimensionality reduction
        self.umap_params = umap_params.copy()
        self.umap_model: Optional[UMAP] = None

        # Data scaler
        self.scaler = StandardScaler()

        self._X_scaled: Optional[np.ndarray] = None
        self._X_reduced: Optional[np.ndarray] = None
        self._labels: Optional[np.ndarray] = None  # generated by the hdbscan
        self._cluster_centers: Optional[np.ndarray] = None
        self._is_fitted = False
        self._knn_training_metrics: Optional[Dict[str, Any]] = None
        self._best_hdbscan_params: Dict[str, Any] = self.hdbscan_params.copy()
        self._clustering_metrics_cache: Optional[Dict[str, Any]] = None
        self._grid_search_history: List[Dict[str, Any]] = []
        self._umap_effective_params: Dict[str, Any] = self.umap_params.copy()
        self._last_hdbscan_noise_ratio: Optional[float] = None
        self._train_noise_ratio: Optional[float] = None

        self.clustering_param_grid = (
            clustering_param_grid
                if clustering_param_grid is not None
                else None
        )

        logger.info(f"AdaptiveClusteringEngine initialized with HDBSCAN params: {self.hdbscan_params}")
        logger.info(f"kNN predictor params: {self.knn_params}")
        logger.info(f"UMAP params: {self.umap_params}")
        if self.clustering_param_grid is None:
            logger.info("HDBSCAN grid search disabled; using base parameters only.")
        else:
            logger.info(f"HDBSCAN grid search configurations: {self.clustering_param_grid}")

    def fit(self, X: np.ndarray) -> ClusteringResult:
        logger.info(f"Starting HDBSCAN clustering fit on data shape: {X.shape}")
        self._validate_input_data(X)

        try:
            # Scale the data
            self._X_scaled = self.scaler.fit_transform(X)
            logger.info("Data scaling completed")

            # Reduce the dimension
            self._fit_umap()

            # Find the optimal hdbscan hyperparameter
            grid_result = self._run_hdbscan_grid_search()

            self.hdbscan_model = grid_result.model
            self._labels = grid_result.labels  # Fitted result
            self._best_hdbscan_params = grid_result.params  # Parameter
            self._clustering_metrics_cache = grid_result.metrics.copy()  # Metrics
            self._last_hdbscan_noise_ratio = None

            noise_ratio = self._clustering_metrics_cache.get('noise_ratio', 0.0)
            self._train_noise_ratio = float(noise_ratio)
            self._last_hdbscan_noise_ratio = self._train_noise_ratio
            logger.info(
                f"Grid search selected params: {self._best_hdbscan_params} "
                f"(noise ratio {noise_ratio:.2%}, score {grid_result.score:.4f})"
            )

            n_clusters = len(np.unique(self._labels[self._labels != -1])) if self._labels is not None else 0
            logger.info(f"HDBSCAN clustering completed with {n_clusters} clusters and {noise_ratio:.2%} noise points")

            self._fit_knn_predictor()
            self._is_fitted = True

            metrics = self.get_clustering_metrics()

            return ClusteringResult(
                labels=self._labels.copy(),
                noise_ratio=noise_ratio,
                metrics=metrics,
            )
        except Exception as exc:
            raise ModelTrainingError(
                f"HDBSCAN clustering failed: {exc}",
                {"model": "HDBSCAN", "training_data_size": int(X.shape[0])},
            ) from exc

    def predict(self, X: np.ndarray) -> np.ndarray:
        if not self._is_fitted:
            raise PredictionError(
                "Model must be fitted before prediction",
                {"prediction_type": "cluster_assignment"},
            )

        logger.info(f"Predicting cluster assignments for data shape: {X.shape}")
        self._validate_input_data(X)

        try:
            X_scaled = self.scaler.transform(X)
            if self.umap_model is None:
                raise PredictionError(
                    "UMAP model is not available for transformation",
                    {"prediction_type": "cluster_assignment", "batch_size": int(X.shape[0])},
                )

            X_reduced = self.umap_model.transform(X_scaled)

            if not self.knn_predictor.is_fitted:
                raise PredictionError(
                    "kNN prediction model must be fitted before prediction",
                    {"prediction_type": "cluster_assignment", "batch_size": int(X.shape[0])},
                )

            labels, metrics = self.knn_predictor.predict(X_reduced)
            avg_conf = metrics.get('avg_confidence', 0.0)
            logger.info(
                f"kNN prediction completed. "
                f"Assigned {len(labels)} points (avg confidence {avg_conf:.4f})",
            )
            return labels
        except Exception as exc:
            raise PredictionError(
                f"Cluster prediction failed: {exc}",
                {"prediction_type": "cluster_assignment", "batch_size": int(X.shape[0])},
            ) from exc

    def get_clustering_metrics(self) -> Dict[str, Any]:
        if not self._is_fitted:
            raise PredictionError(
                "Model must be fitted before calculating metrics",
                {"prediction_type": "clustering_metrics"},
            )

        try:
            if self._clustering_metrics_cache is None:
                if self._X_reduced is None or self._labels is None:
                    raise PredictionError(
                        "No clustering data available for metric calculation",
                        {"prediction_type": "clustering_metrics"},
                    )
                metrics = HDBSCANGridSearch.compute_metrics(self._X_reduced, self._labels)
                metrics['selected_hdbscan_params'] = self._best_hdbscan_params.copy()
                metrics['umap_effective_params'] = self._umap_effective_params.copy()
                self._clustering_metrics_cache = metrics

            return self._clustering_metrics_cache.copy()
        except Exception as exc:
            raise PredictionError(
                f"Failed to calculate clustering metrics: {exc}",
                {"prediction_type": "clustering_metrics"},
            ) from exc

    def _validate_input_data(self, X: np.ndarray) -> None:
        if not isinstance(X, np.ndarray):
            raise ValidationError("Input data must be a numpy array", {"data_type": type(X).__name__})

        if X.ndim != 2:
            raise ValidationError(
                "Input data must be 2-dimensional",
                {"expected_shape": "(n_samples, n_features)", "actual_shape": X.shape},
            )

        if X.shape[0] == 0:
            raise ValidationError("Input data cannot be empty", {"actual_shape": X.shape})

        if X.shape[1] == 0:
            raise ValidationError("Input data must have at least one feature", {"actual_shape": X.shape})

        if np.any(np.isnan(X)):
            raise ValidationError("Input data contains NaN values")

        if np.any(np.isinf(X)):
            raise ValidationError("Input data contains infinite values")

    def _fit_umap(self) -> None:
        if self._X_scaled is None:
            raise ModelTrainingError(
                "Scaled data is not available for UMAP fitting",
            )
        effective_params = self.umap_params.copy()
        n_samples = self._X_scaled.shape[0]

        random_state = effective_params.get('random_state', None)
        if random_state is not None:
            logger.info(
                "Removing UMAP random_state=%s to enable parallel execution",
                random_state,
            )
            effective_params.pop('random_state', None)

        max_neighbors = max(2, n_samples - 1)
        requested_neighbors = effective_params.get('n_neighbors', 15)
        if requested_neighbors >= n_samples:
            effective_params['n_neighbors'] = int(max_neighbors)
            logger.info(
                f"Adjusted UMAP n_neighbors from {requested_neighbors} to {max_neighbors} "
                f"due to sample size {n_samples}"
            )

        max_components = max(2, n_samples - 1)
        requested_components = effective_params.get('n_components', 128)
        if requested_components >= n_samples:
            effective_params['n_components'] = int(max_components)
            logger.info(
                f"Adjusted UMAP n_components from {requested_components} to {max_components} "
                f"due to sample size {n_samples}"
            )

        self.umap_model = UMAP(**effective_params)
        self._umap_effective_params = effective_params
        self._X_reduced = self.umap_model.fit_transform(self._X_scaled)
        logger.info(f"UMAP effective params used for fit: {self._umap_effective_params}")
        logger.info(
            f"UMAP reduction completed. Original dims: {self._X_scaled.shape[1]}, "
            f"Reduced dims: {self._X_reduced.shape[1]}"
        )

    def _run_hdbscan_grid_search(self) -> GridSearchResult:
        if self._X_reduced is None:
            raise ModelTrainingError(
                "Reduced data is not available for HDBSCAN grid search",
            )

        search = HDBSCANGridSearch(self.hdbscan_params, self.clustering_param_grid)
        result = search.evaluate(self._X_reduced)
        self._grid_search_history = [
            {
                'attempt': attempt.index,
                'params': attempt.params,
                'metrics': attempt.metrics,
                'score': attempt.score,
            }
            for attempt in result.history
        ]
        return result

    def _fit_knn_predictor(self) -> None:
        try:
            if self._labels is None:
                raise ModelTrainingError(
                    "kNN prediction model cannot be trained before clustering completes",
                    {"model": "kNN_predictor", "training_data_size": 0},
                )

            if self._X_reduced is None:
                raise ModelTrainingError(
                    "kNN prediction model cannot be trained before the UMAP feature reduction",
                    {"model": "kNN_predictor", "training_data_size": 0}
                )

            self._knn_training_metrics = self.knn_predictor.fit(self._X_reduced, self._labels)
            accuracy = self._knn_training_metrics.get('training_accuracy', 0.0)
            logger.info(f"kNN prediction model fitted successfully with accuracy: {accuracy:.4f}")
        except Exception as exc:
            logger.error(f"Failed to fit kNN prediction model: {exc}")
            self._knn_training_metrics = None
            raise

    def _calculate_cluster_centers(self) -> Optional[np.ndarray]:
        if self._labels is None:
            return None

        data_source = self._X_reduced if self._X_reduced is not None else self._X_scaled
        if data_source is None:
            return None

        unique_labels = np.unique(self._labels)
        valid_labels = unique_labels[unique_labels != -1]
        if len(valid_labels) == 0:
            return np.empty((0, data_source.shape[1]))

        centers = [
            np.mean(data_source[self._labels == label], axis=0)
            for label in valid_labels
        ]
        return np.asarray(centers)

    @property
    def is_fitted(self) -> bool:
        return self._is_fitted

    def get_fitted_result(self) -> ClusteringResult:
        """
        Return the latest ClusteringResult produced during fitting.

        Raises:
            PredictionError: If the engine has not been fitted yet.
        """
        if not self._is_fitted or self._labels is None:
            raise PredictionError(
                "Model must be fitted before accessing fitted results",
                {"prediction_type": "cluster_assignment"},
            )

        metrics = self.get_clustering_metrics()
        labels = self._labels.tolist()
        noise_ratio = (
            float(self._train_noise_ratio)
            if self._train_noise_ratio is not None
            else float(metrics.get('noise_ratio', 0.0))
        )

        return ClusteringResult(
            labels=labels,
            noise_ratio=noise_ratio,
            metrics=metrics,
        )

    @property
    def hdbscan_noise_ratio(self) -> Optional[float]:
        return self._last_hdbscan_noise_ratio
